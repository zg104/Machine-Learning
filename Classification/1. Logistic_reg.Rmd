---
title: "Logistic Regression"
author: "Zijing Gao"
date: "2020/2/9"
output: html_document
---

# Logistic Regression

## we continue discussion of classification


```{r}
library(ISLR)
library(tibble)
as_tibble(Default)
```

```{r}
# split 

set.seed(42)
default_idx = sample(nrow(Default), 5000)
default_trn = Default[default_idx, ]
default_tst = Default[-default_idx, ]
```

## review of LR

```{r}
default_trn_lm = default_trn
default_tst_lm = default_tst
```

Since the LR need the response to be numeric, we coerce the response to be numeric.

```{r}
# we require 0 and 1, not 1 and 2
default_trn_lm$default = as.numeric(default_trn_lm$default) - 1
default_tst_lm$default = as.numeric(default_tst_lm$default) - 1
```

```{r}
model_lm = lm(default ~ balance, data = default_trn_lm)
```


```{r}
plot(default ~ balance, data = default_trn_lm, 
     col = "darkorange", pch = "|", ylim = c(-0.2, 1),
     main = "Using Linear Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
abline(model_lm, lwd = 3, col = "dodgerblue")
```


Two issues arise. First, all of the predicted probabilities are below 0.5. That means, we would classify every observation as a "No". This is certainly possible, but not what we would expect.


```{r}
all(predict(model_lm) < 0.5)
```


```{r}
any(predict(model_lm)<0)
```

## Bayes Classifier

It is seeking to minimize the classification errors.

## Logistic Regression with glm()

```{r}
model_glm = glm(default ~ balance, data = default_trn, family = "binomial")

# family = "binomial" --> we have a two-class categorical response.
``` 

```{r}
# obtain the fitted coef the same way as we did in LR
coef(model_glm)
```

```{r}
# Next, we should know how the predict() works with glm()

head(predict(model_glm))
```

```{r}
head(predict(model_glm), type = "link") # by default
```

These are not predicted probabilities.

```{r}
head(predict(model_glm, type = "response"))
```


Note that these are probabilities, not classifications. To obtain classifications, we will need to compare to the correct cutoff value with an ifelse() statement.


```{r}
model_glm_pred = ifelse(predict(model_glm, type = "link") >0 , "Yes", "No")
```


Once we have classifications, we can calculate metrics such as the trainging classification error rate.

```{r}
calc_class_err = function(actual,predicted){
  mean(actual != predicted) # important
}
```

```{r}
calc_class_err(actual = default_trn$default, predicted = model_glm_pred)
```

As we saw previously, the table() and confusionMatrix() functions can be used to quickly obtain many more metrics.

```{r}
train_tab = table(predicted = model_glm_pred, actual = default_trn$default)

library(caret)
train_con_mat = confusionMatrix(train_tab, positive = "Yes")
c(train_con_mat$overall["Accuracy"], 
  train_con_mat$byClass["Sensitivity"], # True positive rate
  train_con_mat$byClass["Specificity"]) # True negative rate
```

We could also write a custom function for the error for use with trained logist regression models.

```{r}
get_logistic_error = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  preds = ifelse(probs > cut, pos, neg)
  calc_class_err(actual = data[, res], predicted = preds)
}
```


This function will be useful later when calculating train and test errors for several models at the same time.

```{r}
get_logistic_error(model_glm, data = default_trn, 
                   res = "default", pos = "Yes", neg = "No", cut = 0.5)
```

To see how much better logistic regression is for this task, we create the same plot we used for linear regression.





```{r}
plot(default ~ balance, data = default_trn_lm, 
     col = "darkorange", pch = "|", ylim = c(-0.2, 1),
     main = "Using Logistic Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)
curve(predict(model_glm, data.frame(balance = x), type = "response"), 
      add = TRUE, lwd = 3, col = "dodgerblue")
abline(v = -coef(model_glm)[1] / coef(model_glm)[2], lwd = 2) # decision boundary, the balance that obtains a predicted probability of 0.5
```


```{r}
plot(default ~ balance, data = default_trn_lm, 
     col = "darkorange", pch = "|", ylim = c(-0.2, 1),
     main = "Using Logistic Regression for Classification")
grid = seq(0, max(default_trn$balance), by = 0.01)

# using sigmoid function --> f_hat
sigmoid = function(x) { 
  1 / (1 + exp(-x))
}

lines(grid, sigmoid(coef(model_glm)[1] + coef(model_glm)[2] * grid), lwd = 3)
```


Using the usual formula syntax, it is easy to add or remove complexity from logistic regressions.

```{r}
model_1 = glm(default ~ 1, data = default_trn, family = "binomial")
model_2 = glm(default ~ ., data = default_trn, family = "binomial")
model_3 = glm(default ~ . ^ 2 + I(balance ^ 2),
              data = default_trn, family = "binomial")
```


```{r}
model_list = list(model_1, model_2, model_3)
train_errors = sapply(model_list, get_logistic_error, data = default_trn, 
                      res = "default", pos = "Yes", neg = "No", cut = 0.5)
test_errors  = sapply(model_list, get_logistic_error, data = default_tst, 
                      res = "default", pos = "Yes", neg = "No", cut = 0.5)

data.frame(trn_err = train_errors,tst_err = test_errors)
```


Here we see the misclassification error rates for each model. The train decreases, and the test decreases, until it starts to increases. Everything we learned about the bias-variance tradeoff for regression also applies here.

```{r}
diff(train_errors)
```

```{r}
diff(test_errors)
```

We call model_2 the additive logistic model, which we will use quite often.


## ROC curves

Let’s return to our simple model with only balance as a predictor.

```{r}
model_glm = glm(default ~ balance, data = default_trn, family = "binomial")
```


```{r}
get_logistic_pred = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  ifelse(probs > cut, pos, neg)
}
```

Let’s use this to obtain predictions using a low, medium, and high cutoff. (0.1, 0.5, and 0.9)


```{r}
test_pred_10 = get_logistic_pred(model_glm, data = default_tst, res = "default", 
                                 pos = "Yes", neg = "No", cut = 0.1)
test_pred_50 = get_logistic_pred(model_glm, data = default_tst, res = "default", 
                                 pos = "Yes", neg = "No", cut = 0.5)
test_pred_90 = get_logistic_pred(model_glm, data = default_tst, res = "default", 
                                 pos = "Yes", neg = "No", cut = 0.9)
```

Now we evaluate accuracy, sensitivity, and specificity for these classifiers.

```{r}
test_tab_10 = table(predicted = test_pred_10, actual = default_tst$default)
test_tab_50 = table(predicted = test_pred_50, actual = default_tst$default)
test_tab_90 = table(predicted = test_pred_90, actual = default_tst$default)

test_con_mat_10 = confusionMatrix(test_tab_10, positive = "Yes")
test_con_mat_50 = confusionMatrix(test_tab_50, positive = "Yes")
test_con_mat_90 = confusionMatrix(test_tab_90, positive = "Yes")
```

```{r}
metrics = rbind(
  
  c(test_con_mat_10$overall["Accuracy"], 
    test_con_mat_10$byClass["Sensitivity"], 
    test_con_mat_10$byClass["Specificity"]),
  
  c(test_con_mat_50$overall["Accuracy"], 
    test_con_mat_50$byClass["Sensitivity"], 
    test_con_mat_50$byClass["Specificity"]),
  
  c(test_con_mat_90$overall["Accuracy"], 
    test_con_mat_90$byClass["Sensitivity"], 
    test_con_mat_90$byClass["Specificity"])

)

rownames(metrics) = paste("c =", c(0.1,0.5,0.9))
metrics
```

Instead of manually checking cutoffs, we can create an ROC curve (receiver operating characteristic curve) which will sweep through all possible cutoffs, and plot the sensitivity and specificity.

```{r}
library(pROC)
test_prob = predict(model_glm, newdata = default_tst, type = "response")
test_roc = roc(default_tst$default ~ test_prob, plot = TRUE, print.auc = TRUE)
```

```{r}
as.numeric(test_roc$auc)
```

A good model will have a high AUC, that is as often as possible a high sensitivity and specificity.


## Multinomial Logistic Regression



What if the response contains more than two categories? For that we need multinomial logistic regression.


```{r}
# split
set.seed(430)
iris_obs = nrow(iris)
iris_idx = sample(iris_obs, size = trunc(0.50 * iris_obs))
iris_trn = iris[iris_idx, ]
iris_test = iris[-iris_idx, ]
```


To perform multinomial logistic regression, we use the multinom function from the nnet package. Training using multinom() is done using similar syntax to lm() and glm(). We add the trace = FALSE argument to suppress information about updates to the optimization routine as the model is trained.

```{r}
library(nnet)
model_multi = multinom(Species ~ ., data = iris_trn, trace = FALSE)
summary(model_multi)$coefficients
```

Notice we are only given coefficients for two of the three class, much like only needing coefficients for one class in logistic regression.

A difference between glm() and multinom() is how the predict() function operates.

```{r}
head(predict(model_multi, newdata = iris_trn))
```


```{r}
head(predict(model_multi, newdata = iris_trn, type = "prob"))
```






