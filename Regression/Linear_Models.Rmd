---
title: "Linear Models"
author: "Zijing Gao"
date: "2/8/2020"
output:
  word_document: default
  html_document: default
---










# Linear Models


## Visualization for Regression

```{r}
library(readr)
Advertising = read_csv("Advertising.csv")
Advertising
```

```{r}
pairs(Advertising)
```


## lm() function

```{r}
mod_1 = lm(Sales ~ ., data = Advertising)
# mod_1 = lm(Sales ~ TV + Radio + Newspaper, data = Advertising)
```

## Hypothesis Testing

```{r}
summary(mod_1)
```

```{r}
mod_0 = lm(Sales ~ TV  + Radio, data = Advertising)
```

The anova() function is useful for comparing __two__ models. Here we compare the full additive model, mod_1, to a reduced model mod_0. Essentially we are testing for the significance of the __Newspaper__ variable in the additive model.

```{r}
anova(mod_0, mod_1)
```

## prediction

```{r}
head(predict(mod_1), n = 10)
```

```{r}
new_obs = data.frame(TV = 150, Radio = 40, Newspaper = 1)
```


We can then use the predict() function for point estimates, confidence intervals, and prediction intervals.

Using only the first two arguments, R will simply return a point estimate, that is, the “predicted value,” $\over y$

```{r}
predict(mod_1,newdata = new_obs)
```

If we specify an additional argument interval with a value of "confidence", R will return a 95% confidence interval for the mean response at the specified point. Note that here R also gives the point estimate as fit.

```{r}
predict(mod_1, newdata = new_obs, interval = "confidence")
```

Lastly, we can alter the level using the level argument. Here we report a prediction interval instead of a confidence interval.

```{r}
predict(mod_1, newdata = new_obs, interval = "prediction", level = 0.99)
```

## Unusual Observations


R provides several functions for obtaining metrics related to unusual observations.

1 resid() provides the residual for each observation
2 hatvalues() gives the leverage of each observation
3 rstudent() give the studentized residual for each observation
4 cooks.distance() calculates the influence of each observation

```{r}
head(resid(mod_1),10)
```

```{r}
head(hatvalues(mod_1),10)
```

```{r}
head(rstudent(mod_1),10)
```

```{r}
head(cooks.distance(mod_1),10)
```


When using linear models in the past, we often emphasized distributional results, which were useful for creating and performing hypothesis tests. Frequently, when developing a linear regression model, part of our goal was to explain a relationship.

Now, we will ignore much of what we have learned and instead simply use regression as a tool to predict. Instead of a model which explains relationships, we seek a model which minimizes errors.

## Assesing Model Accuracy



```{r}
library(caret)
featurePlot(x = Advertising[ , c("TV", "Radio", "Newspaper")], y = Advertising$Sales)
```

```{r}
rmse = function(actual, predicted){
  sqrt(mean((actual-predicted)^2))
}

```


## Model Complexity (flexibiliy)

Aside from how well a model predicts, we will also be very interested in the complexity (flexibility) of a model. For now, we will only consider nested linear models for simplicity. Then in that case, the more predictors that a model has, the more complex the model. For the sake of assigning a numerical value to the complexity of a linear model, we will use the number of predictors,  
p
 .

```{r}
get_complexity = function(model){
  length(coef(model))-1
}

```

## Test-Train Split

```{r}
set.seed(9)
num_obs = nrow(Advertising)

train_index = sample(num_obs, size = trunc(0.50 * num_obs))
train_data = Advertising[train_index,]
test_data = Advertising[-train_index,]
```

Note that there are two measures that assess how well a model is predicting, the __train RMSE__ and __test RMSE__


Test RMSE uses the model fit to the training data, but evaluated on the unused test data. This is a measure of how well the fitted model will predict in general, not simply how well it fits data used to train the model, as is the case with train RMSE. What happens to test RMSE as the size of the model increases? That is what we will investigate.

We will start with the simplest possible linear model, that is, a model with no predictors.


```{r}
fit_0 = lm(Sales ~ 1, data = train_data)
get_complexity(fit_0)
```

```{r}
# train RMSE
sqrt(mean((train_data$Sales-predict(fit_0, train_data))^2))
```

```{r}
# test RMSE
sqrt(mean((test_data$Sales-predict(fit_0, test_data))^2))
```


```{r}
# improve

get_rmse = function(model, data, response){
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}

get_rmse(model = fit_0, data = train_data, response = "Sales") # train RMSE

get_rmse(model = fit_0, data = test_data, response = "Sales") # test RMSE
```


## Adding Flexibility to Linear Models

Each successive model we fit will be more and more flexible using both interactions and polynomial terms. We will see the training error decrease each time the model is made more flexible. We expect the test error to decrease a number of times, then eventually start going up, as a result of overfitting.


```{r}
fit_1 = lm(Sales ~ ., data = train_data)
get_complexity(fit_1)
```


```{r}
get_rmse(model = fit_1, data = train_data, response = "Sales") # train RMSE

get_rmse(model = fit_1, data = test_data, response = "Sales") # test RMSE
```


```{r}
fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)
get_complexity(fit_2)
```

```{r}
get_rmse(model = fit_2, data = train_data, response = "Sales") # train RMSE

get_rmse(model = fit_2, data = test_data, response = "Sales") # test RMSE
```


```{r}
fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)
get_complexity(fit_3)

```

```{r}
get_rmse(model = fit_3, data = train_data, response = "Sales") # train RMSE

get_rmse(model = fit_3, data = test_data, response = "Sales") # test RMSE
```


```{r}
fit_4 = lm(Sales ~ Radio * Newspaper * TV + 
           I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)
get_complexity(fit_4)

```

```{r}
get_rmse(model = fit_4, data = train_data, response = "Sales") # train RMSE

get_rmse(model = fit_4, data = test_data, response = "Sales") # test RMSE
```

```{r}
fit_5 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)
get_complexity(fit_5)
```


```{r}
get_rmse(model = fit_5, data = train_data, response = "Sales") # train RMSE

get_rmse(model = fit_5, data = test_data, response = "Sales") # test RMSE
```


## choosing a model

```{r}
fit_1 = lm(Sales ~ ., data = train_data)
fit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)
fit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)
fit_4 = lm(Sales ~ Radio * Newspaper * TV + 
           I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)
fit_5 = lm(Sales ~ Radio * Newspaper * TV +
           I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)

```


```{r}
model_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)
```

```{r}
train_rmse = sapply(model_list, get_rmse, data = train_data, response = "Sales")
test_rmse = sapply(model_list, get_rmse, data = test_data, response = "Sales")
model_complexity = sapply(model_list, get_complexity)
```


```{r}
# plot

plot(model_complexity, train_rmse, type = "b", 
     ylim = c(min(c(train_rmse, test_rmse)) - 0.02, 
              max(c(train_rmse, test_rmse)) + 0.02), 
     col = "dodgerblue", 
     xlab = "Model Size",
     ylab = "RMSE")
lines(model_complexity, test_rmse, type = "b", col = "darkorange")
```


```{r}
# summerize

result_LM = matrix(0,nrow = 5, ncol = 4)
result_LM[,1] = paste("fit", 1:5, sep = "_")
result_LM[,2] = round(train_rmse,8)
result_LM[,3] = round(test_rmse,8)
result_LM[,4] = model_complexity
colnames(result_LM) = c("Model", "Train RMSE", "Test RMSE", "Predictors")
result_LM = as.data.frame(result_LM)
result_LM
```

We can tell that fit_1 and fit_2 are Underfitting becasue of High Train RMSE and High Test RMSE.

fit_1 and fit_2 are overfitting becasue of low Train RMSE and High Test RMSE.

__Specifically, we say that a model is overfitting if there exists a less complex model with lower Test RMSE. Then a model is underfitting if there exists a more complex model with lower Test RMSE.__



A number of notes on these results:

-The labels of under and overfitting are relative to the best model we see, fit_3. Any model more complex with higher Test RMSE is overfitting. Any model less complex with higher Test RMSE is underfitting.

-The train RMSE is guaranteed to follow this non-increasing pattern. The same is not true of test RMSE. Here we see a nice U-shaped curve. There are theoretical reasons why we should expect this, but that is on average. Because of the randomness of one test-train split, we may not always see this result. Re-perform this analysis with a different seed value and the pattern may not hold. We will discuss why we expect this next chapter. We will discuss how we can help create this U-shape much later.

-Often we expect train RMSE to be lower than test RMSE. Again, due to the randomness of the split, you may get lucky and this will not be true.


A final note on the analysis performed here; we paid no attention whatsoever to the “assumptions” of a linear model. We only sought a model that predicted well, and paid no attention to a model for explaination. Hypothesis testing did not play a role in deciding the model, only prediction accuracy. Collinearity? We don’t care. Assumptions? Still don’t care. Diagnostics? Never heard of them. (These statements are a little over the top, and not completely true, but just to drive home the point that we only care about prediction. Often we latch onto methods that we have seen before, even when they are not needed.)
